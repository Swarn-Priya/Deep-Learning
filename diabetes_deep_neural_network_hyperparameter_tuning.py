# -*- coding: utf-8 -*-
"""Diabetes - Deep Neural Network - Hyperparameter Tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aHp35yiobSNL45M5EvLRTaTZimjhhBcs
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set()
import warnings
warnings.filterwarnings('ignore')

df=pd.read_csv('/content/diabetes.csv')
df.head(1)

df.info()

for i in df.columns:
    print("********",i,"***********")
    print(set(df[i].tolist()))
    print()
    print()

#  'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI'  ---- Can't be ZERO. We will have to replace these zeros
# with median value. Else it will be trained wrong and will give wrong result.

zero = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

for i in zero:
    df[i] = np.where(df[i] == 0,df[i].median(),df[i])

for i in df.columns:
    print("*****",i,"**********")
    print(set(df[i].tolist()))
    print()
    print()

# Pre-Processing of the data

# 1. Missing Data

df.isnull().sum()

# 2. Encoding not required. ALl data in either in int or float format.
# 3. no outliers as well.
df.describe()

# 4. Imbalance dataset

df['Outcome'].value_counts()

268*2  # not imbalance.

x = df.iloc[:,0:-1]
y=df.iloc[:,-1]

# 5. Feature Scaling.

from sklearn.preprocessing import StandardScaler
sc= StandardScaler()
scx = sc.fit_transform(x)
scx.shape

#  Spliting the data into training and testing datasets.

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(scx,y,test_size=0.2,random_state=1)
print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)

import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout

model = Sequential()
model.add(Dense(6, activation='relu',input_dim=8))
model.add(Dropout(0.5))
model.add(Dense(1,activation='sigmoid'))
model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

"""# **Hyperparameter Tuning can be done?**

1. How many hidden layers can be given?
2. How many neurons are required?
3. What is the best activation model?
4. Which optimization would be the best?
5. What is the Dropout percent we can give?
"""

model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=100)

"""We can see basic model is unable to give us good accuracy -- it is only 735 and 78% ...even after 100 epochs....

Hence, now we will try to do hyperparameter tuning and will see how it goes.

# **HYPERPARAMETER **
"""

!pip install -U keras-tuner

import kerastuner as kt

# 1. Optimizer

def build_model(hp):
  model=Sequential()
  model.add(Dense(6,activation='relu',input_dim=8))
  model.add(Dropout(0.5))
  model.add(Dense(1,activation='sigmoid'))
  optimizer = hp.Choice('optimizer',values = ['sgd','adadelta','adagrade','rmsprop','adam','nadam'])
  model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])
  return model

# Best Optimizer.

tuner = kt.RandomSearch(build_model,objective='val_loss',max_trials=5)

tuner.search(x_train,y_train,epochs=5,validation_data=(x_test,y_test))

tuner.get_best_hyperparameters()[0].values

model = tuner.get_best_models(num_models=1)[0]

model.summary()

model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=100,initial_epoch=6)

"""# **All Hyper parameter Tunning. **"""

def build_model(hp):
  model= Sequential()
  counter = 0

  for i in range(hp.Int('num layers',min_value=1,max_value=10)):
    if counter==0:
      model.add(Dense(hp.Int('units'+str(i),min_value=6,max_value=128),
                      activation = hp.Choice("activation"+str(i),
                                             values=['relu','tanh','sigmoid','elu','leaky_relu','selu','linear']),
                      input_dim=8))
      model.add(Dropout(hp.Choice('Dropout'+str(i),values=[0.1,0.2,0.3,0.4,0.5,0.6])))
    else:
      model.add(Dense(hp.Int('units'+str(i),min_value=6,max_value=128),
                      activation=hp.Choice('activation'+str(i),values=['relu','tanh','sigmoid',
                                                                       'elu','leaky_relu','selu','linear'])))
      model.add(Dropout(hp.Choice('Dropout'+str(i),values=[0.1,0.2,0.3,0.4,0.5,0.6])))
    counter += 1
  model.add(Dense(1,activation='sigmoid'))
  model.compile(optimizer=hp.Choice("optimizer",values=['rmsprop','adam','nadam','sgd','adadelta',
                                                          'adagrad']),loss='binary_crossentropy',
                  metrics=['accuracy'])

  return model

tuner = kt.RandomSearch(build_model,objective='val_loss',max_trials =5,directory = 'my_directory',
                        project_name='Diabetics_Project')

tuner.search(x_train,y_train,epochs=5,validation_data=(x_test,y_test))

tuner.get_best_hyperparameters()[0].values

tuner.get_best_models(num_models = 1)[0]

model=tuner.get_best_models(num_models=5)[0]

model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=100,initial_epoch=6)







